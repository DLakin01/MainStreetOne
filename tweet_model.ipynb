{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Gender Classification with NLP, Scikit-Learn, and LightGBM\n",
    "#### *Daniel Lakin*\n",
    "---\n",
    "In this notebook, we use various a combination of machine learning and linguistic processing tools to build\n",
    "a model that can predict, given an individual tweet, the author's gender. Working from an archived set of files,\n",
    "we extract raw data on tweets from 200 individual users, and supplement the data with a call to Twitter's API. We\n",
    "then extract new linguistic and categorical features on each tweet, working primarily from the posted text.\n",
    "\n",
    "After feature extraction and scaling, we feed it into a series of Microsoft LightGBM Gradient Boosting Classifiers,\n",
    "taking advantage of the power and speed offered by the algorithm. The models are tuned using a randomized search\n",
    "cross-validation approach, and we explore the possible benefits of enhancing our results with grid search.\n",
    "\n",
    "To run the below code, please place the archive with the tweet data (mso_ds_interview.tgz) in the same directory as this notebook, and ensure you have the following Python packages installed, either via pip or in a conda environment:\n",
    "\n",
    "- matplotlib\n",
    "- spacymoji\n",
    "- lightgbm\n",
    "- textblob\n",
    "- jupyter\n",
    "- seaborn\n",
    "- sklearn\n",
    "- requests\n",
    "- pandas\n",
    "- numpy\n",
    "- spacy\n",
    "- shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import warnings\n",
    "import tarfile\n",
    "import random\n",
    "import spacy\n",
    "import json\n",
    "import shap\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve, average_precision_score\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from emoji import UNICODE_EMOJI, UNICODE_EMOJI_ALIAS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tarfile import ExFileObject\n",
    "from textblob import TextBlob\n",
    "from itertools import product\n",
    "from spacymoji import Emoji\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib inline\n",
    "shap.initjs()\n",
    "\n",
    "TWEET_COLUMNS = [\"id\", \"combined_text\", \"language\", \"num_mentions\", \"num_hashtags\", \"gender\"]\n",
    "TWITTER_ROOT_URL = \"https://api.twitter.com\"\n",
    "TWITTER_CONSUMER_KEY = \"3RZxLkkQFDMnN3epDPOcP61hP\"\n",
    "TWITTER_CONSUMER_SECRET = \"cwfoe7umOC4tAIJE4VmirEjmQE2NPWlnfCr91jS0EQUiOB4cNk\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section, we unzip the archive containing our data, and parse the `manifest.jl` file, which contains\n",
    "unique IDs for each user in the dataset, as well as their gender, denoted by M for male and F for female. We'll be\n",
    "using the gender data as our label data later in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_julia_file(tarfile: ExFileObject):\n",
    "    tar_string = tarfile.read().decode()\n",
    "    return [json.loads(el) for el in tar_string.split(\"\\n\") if el != \"\"]\n",
    "\n",
    "\n",
    "tar = tarfile.open(\"mso_ds_interview.tgz\", \"r\")\n",
    "manifest = tar.extractfile(\"ds_interview/manifest.jl\")\n",
    "manifest_details = parse_julia_file(manifest)\n",
    "pprint(manifest_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the unique IDs extracted from manifest.jl, we can then use the same parse_julia_file method defined above to\n",
    "pull the information for each of the 200 users. For each user, we also pull their profile description via the\n",
    "Twitter API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data_list = []\n",
    "\n",
    "# Authenticate to Twitter API\n",
    "auth = HTTPBasicAuth(\"3RZxLkkQFDMnN3epDPOcP61hP\", \"cwfoe7umOC4tAIJE4VmirEjmQE2NPWlnfCr91jS0EQUiOB4cNk\")\n",
    "auth_response = requests.post(\"https://api.twitter.com/oauth2/token?grant_type=client_credentials\", auth=auth).json()\n",
    "headers = {\"Authorization\": f\"Bearer {auth_response['access_token']}\"}\n",
    "\n",
    "\n",
    "def parse_tweet_text(tweet_obj: dict):\n",
    "    try:\n",
    "        text = tweet_obj[\"full_text\"]\n",
    "        if text == \"\":\n",
    "            text = tweet_obj[\"text\"]\n",
    "\n",
    "    except KeyError:\n",
    "        text = tweet_obj[\"text\"]\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "with requests.session() as session:\n",
    "    session.headers = headers\n",
    "\n",
    "    for user in manifest_details:\n",
    "        user_id = user[\"user_id_str\"]\n",
    "        print(f\"Pulling tweets for user {user_id}\")\n",
    "\n",
    "        api_result = session.get(f\"https://api.twitter.com/1.1/users/lookup.json?user_id={user_id}\").json()\n",
    "        if \"errors\" in api_result:\n",
    "            profile_description = \"\"\n",
    "\n",
    "        else:\n",
    "            profile_description = api_result[0][\"description\"]\n",
    "\n",
    "        tweet_file = tar.extractfile(f\"ds_interview/tweet_files/{user_id}.tweets.jl\")\n",
    "        user_tweets = parse_julia_file(tweet_file)\n",
    "\n",
    "        for t in user_tweets:\n",
    "            tdoc = t[\"document\"]\n",
    "            tweet_id = tdoc[\"id\"]\n",
    "            combined_text = parse_tweet_text(tdoc) + \" \" + profile_description\n",
    "            language = tdoc[\"lang\"]\n",
    "            num_mentions = len(tdoc[\"entities\"][\"user_mentions\"])\n",
    "            num_hashtags = len(tdoc[\"entities\"][\"hashtags\"])\n",
    "\n",
    "            tweets_data_list.append([\n",
    "                tweet_id,\n",
    "                combined_text,\n",
    "                language,\n",
    "                num_mentions,\n",
    "                num_hashtags,\n",
    "                user[\"gender_human\"]\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've pulled and processed data on all the tweets, let's assemble it all into a Pandas Dataframe and take a look at what we have. As we can see below, the dataframe contains the following columns:\n",
    "\n",
    "- Unique tweet ID\n",
    "- Text of tweet + the user's profile description\n",
    "- The language the tweet was written in\n",
    "- The number of mentions in the tweet\n",
    "- The number of hashtags in the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.DataFrame(tweets_data_list, columns=TWEET_COLUMNS)\n",
    "tweets_df.set_index(\"id\", inplace=True)\n",
    "tweets_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have more than one language present in our data! Before proceeding, let's check to see the distribution of languages across the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_counts = tweets_df.groupby(\"language\").count()[\"combined_text\"]\n",
    "language_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are over 40 languges present in the dataset, with English being the vast majority (over\n",
    "88%). While this introduces some complexity, we can cope with it - the excellent NLP library spaCy provides strong\n",
    "support across multiple languages. In the interest of preserving as much of the data as possible, we'll narrow our\n",
    "dataframe to include tweets only in the languages spaCy can \"understand\", and proceed with linguistic feature\n",
    "extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACY_LANGS = [\"de\", \"el\", \"en\", \"es\", \"fr\", \"it\", \"lt\", \"nb\", \"nl\", \"pt\"]\n",
    "\n",
    "# Throw out samples that spaCy can't parse\n",
    "tweets_df = tweets_df[tweets_df[\"language\"].isin(SPACY_LANGS)]\n",
    "\n",
    "language_counts = tweets_df.groupby(\"language\").count()[\"combined_text\"]\n",
    "language_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After narrowing the dataset based on language, we now take another step to reduce the size of the data we're working\n",
    "with. Even with only spaCy-friendly languages, the dataset still contains over 650,000 tweets, which will be very\n",
    "computationally expensive to work with. To make things easier but still keep the modeling process robust, we'll take\n",
    "a 15% sample of the data and use that from now on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = tweets_df.sample(frac=0.15, random_state=42)\n",
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the size of the data reduced to a more manageable level, we can begin our linguistic processing and feature\n",
    "extraction. In the below section, we work with one language group at a time, running each one through a suite of\n",
    "linguistic checks, leveraging spaCy's ability to parse out parts of speech and other features from our chosen\n",
    "languages. We also collect information on the number of contractions and emoticons in each tweet, as well as it's\n",
    "sentiment (positive, neutral, negative) as detected by the Textblob library's sentiment parser.\n",
    "\n",
    "I've chosen to collect feature information on all the parts of speech that spaCy can recognize in the languages indicated above, as well as tweet sentiment, on the theory that there are detectable and generalizeable patterns in language usage that will allow a model to tell the gender of a person by looking at their tweet. This step will take ~3 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTIONS = [\"ain't\", \"aren't\", \"can't\", \"can't've\", \"'cause\", \"could've\", \"couldn't\", \"couldn't've\", \"didn't\",\n",
    "                \"doesn't\", \"don't\", \"hadn't\", \"hadn't've\", \"hasn't\", \"haven't\", \"he'd\", \"he'd've\", \"he'll\", \"he'll've\",\n",
    "                \"he's\", \"how'd\", \"how'd'y\", \"how'll\", \"how's\", \"I'd\", \"I'd've\", \"I'll\", \"I'll've\", \"I'm\", \"I've\", \"i'd\",\n",
    "                \"i'd've\", \"i'll\", \"i'll've\", \"i'm\", \"i've\", \"isn't\", \"it'd\", \"it'd've\", \"it'll\", \"it'll've\", \"it's\",\n",
    "                \"let's\", \"ma'am\", \"mayn't\", \"might've\", \"mightn't\", \"mightn't've\", \"must've\", \"mustn't\", \"mustn't've\",\n",
    "                \"needn't\", \"needn't've\", \"o'clock\", \"oughtn't\", \"oughtn't've\", \"shan't\", \"sha'n't\", \"shan't've\",\n",
    "                \"she'd\", \"she'd've\", \"she'll\", \"she'll've\", \"she's\", \"should've\", \"shouldn't\", \"shouldn't've\", \"so've\",\n",
    "                \"so's\", \"that'd\", \"that'd've\", \"that's\", \"there'd\", \"there'd've\", \"there's\", \"they'd\", \"they'd've\",\n",
    "                \"they'll\", \"they'll've\", \"they're\", \"they've\", \"to've\", \"wasn't\", \"we'd\", \"we'd've\", \"we'll\",\n",
    "                \"we'll've\", \"we're\", \"we've\", \"weren't\", \"what'll\", \"what'll've\", \"what're\", \"what's\", \"what've\",\n",
    "                \"when's\", \"when've\", \"where'd\", \"where's\", \"where've\", \"who'll\", \"who'll've\", \"who's\", \"who've\",\n",
    "                \"why's\", \"why've\", \"will've\", \"won't\", \"won't've\", \"would've\", \"wouldn't\", \"wouldn't've\", \"y'all\",\n",
    "                \"y'all'd\", \"y'all'd've\", \"y'all're\", \"y'all've\", \"you'd\", \"you'd've\", \"you'll\", \"you'll've\", \"you're\",\n",
    "                \"you've\"]\n",
    "POS_MAP = {\n",
    "    \"ADJ\": \"num_adjectives\",\n",
    "    \"ADV\": \"num_adverbs\",\n",
    "    \"CONJ\": \"num_conjunctions\",\n",
    "    \"NOUN\": \"num_nouns\",\n",
    "    \"NUM\": \"num_numerals\",\n",
    "    \"PART\": \"num_particles\",\n",
    "    \"PRON\": \"num_pronouns\",\n",
    "    \"PROPN\": \"num_proper_nouns\",\n",
    "    \"PUNCT\": \"num_punctuation_mks\",\n",
    "    \"VERB\": \"num_verbs\"\n",
    "}\n",
    "\n",
    "\n",
    "def extract_linguistic_features(texts, tweet_ids, spacy_nlp):\n",
    "    all_features = []\n",
    "\n",
    "    for i, doc in enumerate(spacy_nlp.pipe(texts, disable=[\"tagger\", \"parser\", \"ner\"], n_threads=16, batch_size=10000)):\n",
    "        features = {\n",
    "            \"id\": tweet_ids[i],\n",
    "            \"num_words\": len(doc),\n",
    "            \"tweet_length\": len(doc.text),\n",
    "            \"num_exclamation_pts\": doc.text.count(\"!\"),\n",
    "            \"num_question_mks\": doc.text.count(\"?\"),\n",
    "            \"num_periods\": doc.text.count(\".\"),\n",
    "            \"num_hyphens\": doc.text.count(\"-\"),\n",
    "            \"num_capitals\": sum(1 for char in doc.text if char.isupper()),\n",
    "            \"num_emoticons\": sum(\n",
    "                1 for token in doc\n",
    "                if token._.is_emoji\n",
    "                or token in UNICODE_EMOJI\n",
    "                or token in UNICODE_EMOJI_ALIAS\n",
    "            ),\n",
    "            \"num_unique_words\": len(set(token.text for token in doc)),\n",
    "            \"num_adjectives\": 0,\n",
    "            \"num_nouns\": 0,\n",
    "            \"num_pronouns\": 0,\n",
    "            \"num_adverbs\": 0,\n",
    "            \"num_conjunctions\": 0,\n",
    "            \"num_numerals\": 0,\n",
    "            \"num_particles\": 0,\n",
    "            \"num_proper_nouns\": 0,\n",
    "            \"num_verbs\": 0,\n",
    "            \"num_contractions\": 0,\n",
    "            \"num_punctuation_mks\": 0\n",
    "        }\n",
    "\n",
    "        for token in doc:\n",
    "            if token.text in CONTRACTIONS:\n",
    "                features[\"num_contractions\"] += 1\n",
    "\n",
    "            if token.pos_ in POS_MAP:\n",
    "                column_key = POS_MAP[token.pos_]\n",
    "                features[column_key] += 1\n",
    "\n",
    "        clean_tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+://\\S+)\", \" \", doc.text).split())\n",
    "        features[\"sentiment\"] = parse_sentiment(clean_tweet)\n",
    "\n",
    "        all_features.append(features)\n",
    "\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def parse_sentiment(tweet):\n",
    "    \"\"\"\n",
    "    Utility function to classify sentiment of passed tweet\n",
    "    using textblob's sentiment method\n",
    "    \"\"\"\n",
    "\n",
    "    # Create TextBlob object of tweet text\n",
    "    parsed = TextBlob(tweet)\n",
    "    # set sentiment\n",
    "    if parsed.sentiment.polarity > 0:\n",
    "        return 1\n",
    "    elif parsed.sentiment.polarity == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "split_by_lang = [{\"lang\": lang, \"df\": tweets_df[tweets_df[\"language\"] == lang]} for lang in SPACY_LANGS]\n",
    "for item in split_by_lang:\n",
    "    nlp = spacy.load(item[\"lang\"])\n",
    "    emoji = Emoji(nlp, merge_spans=False)\n",
    "    nlp.add_pipe(emoji, first=True)\n",
    "\n",
    "    tweet_ids = item[\"df\"].index.tolist()\n",
    "    texts = item[\"df\"][\"combined_text\"].tolist()\n",
    "    if len(texts):\n",
    "        spacy_features = extract_linguistic_features(texts, tweet_ids, nlp)\n",
    "        temp_df = pd.DataFrame.from_records(spacy_features)\n",
    "        temp_df.set_index(\"id\", inplace=True)\n",
    "\n",
    "        item[\"df\"] = item[\"df\"].merge(temp_df, how=\"left\", on=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all linguistic features have been extracted, we combine the various language-specific dataframes back into\n",
    "one, store the column names for use later, and begin scaling. We've collected a wide range of numeric data, and have categorical data in the form of the Textblob sentiment scores, and all need to be scaled. To do this efficiently, we set up a nested series of Scikit-Learn Pipelines, contained inside a ColumnTransformer, which we then use to scale all our data at once. Our numeric data is scaled using Scikit-Learn's RobustScaler, which helps avoid distortion due to outliers. The\n",
    "categorical sentiment data is transformed using Scikit-Learn's OneHotEncoder.\n",
    "\n",
    "After scaling, we split the data into training and testing sets, using Scikit-Learn's train_test_split utility\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([lang_item[\"df\"] for lang_item in split_by_lang], sort=False)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(X[\"gender\"].map({\"M\": 0, \"F\": 1}))\n",
    "\n",
    "X.drop(columns=[\"gender\", \"language\", \"combined_text\"], inplace=True)\n",
    "column_labels = pd.get_dummies(X, columns=[\"sentiment\"]).columns.tolist()\n",
    "\n",
    "# Scale numeric features\n",
    "numeric_features = [\"num_mentions\", \"num_hashtags\", \"num_nouns\", \"num_pronouns\", \"num_adjectives\", \"num_particles\",\n",
    "                    \"num_words\", \"tweet_length\", \"num_exclamation_pts\", \"num_question_mks\", \"num_periods\", \"num_verbs\",\n",
    "                    \"num_hyphens\", \"num_capitals\", \"num_emoticons\", \"num_unique_words\", \"num_conjunctions\",\n",
    "                    \"num_numerals\", \"num_contractions\", \"num_adverbs\", \"num_proper_nouns\", \"num_punctuation_mks\"]\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"robust\", RobustScaler())\n",
    "])\n",
    "\n",
    "categorical_features = [\"sentiment\"]\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"onehot\", OneHotEncoder(categories=\"auto\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "X = preprocessor.fit_transform(X)\n",
    "\n",
    "# Split into train and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've collected and scaled all our data, we can start using it to train and test models. We'll be using\n",
    "the Light Gradient Boosting Machine (LightGBM) family of algorithms developed by Microsoft, which delivers the full\n",
    "predictive power of Gradient Boosting Classification while offering a considerable speed advantage over the\n",
    "counterparts available from libraries like Scikit-Learn.\n",
    "\n",
    "I've chosen to use Gradient Boosting with cross-validation as my model for this task because of its iterative and\n",
    "self-correcting internal logic. On its own, gradient boosting classification uses multiple (often dozens)\n",
    "rounds of internal decision trees, each working in sequence to make predictions and learn from the trees that came\n",
    "before. By the time the algorithm has reached the end of the sequence of trees defined by the user, the chain of\n",
    "estimators has experimented with the weighting of the features available and arrived at its best guess at how those\n",
    "features are related to the target variable and a model for predicting it.\n",
    "\n",
    "With the addition of k-fold cross-validation, the final model derived from training is made even stronger. In each\n",
    "round of CV, the model has access to k - 1 parts of the data, with the final part being held out as that round's\n",
    "\"test\" set. In this fashion, we can arrive at the best-performing model possible.\n",
    "\n",
    "In the below cell, we define two helper functions that we will use to perform cross-validation with LightGBM. We will also be\n",
    "using early stopping in all boosting processes. Although each round of cross-validation will have up to 1000 boosting\n",
    "estimators available, if the model goes through 50 estimators without improvement, it will end iteration and move on\n",
    "to the next round of CV.\n",
    "\n",
    "To measure model improvement, we will be using the ROC-AUC scoring metric. This measure provides a good indicator of\n",
    "how the model is doing against each set of validation data. It takes into account both the true positive and false\n",
    "positive rate to create a composite score that measures how well the model is doing at distinguishing between positive\n",
    "negative predictions of a Twitter user's gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_lgb_results(hyperparams, iteration):\n",
    "    \"\"\"\n",
    "    Scoring helper for grid and random search. Returns CV score from the given\n",
    "    hyperparameters\n",
    "    \"\"\"\n",
    "\n",
    "    # Find optimal n_estimators using early stopping\n",
    "    if \"n_estimators\" in hyperparams.keys():\n",
    "        del hyperparams[\"n_estimators\"]\n",
    "\n",
    "    # Perform n_folds CV\n",
    "    cv_res = lgb.cv(\n",
    "        params=hyperparams,\n",
    "        train_set=train_set,\n",
    "        num_boost_round=1000,\n",
    "        nfold=5,\n",
    "        early_stopping_rounds=50,\n",
    "        metrics=[\"auc\", \"accuracy\"],\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Return CV results\n",
    "    score = cv_res[\"auc-mean\"][-1]\n",
    "    estimators = len(cv_res[\"auc-mean\"])\n",
    "    hyperparams[\"n_estimators\"] = estimators\n",
    "\n",
    "    return [score, hyperparams, iteration]\n",
    "\n",
    "\n",
    "def light_random_search(param_grid, max_evals=5):\n",
    "    # Dataframe to store results\n",
    "    results = pd.DataFrame(columns=[\"score\", \"params\", \"iteration\"], index=list(range(max_evals)))\n",
    "\n",
    "    # Select max_eval combinations of params to check\n",
    "    for i in range(max_evals):\n",
    "        iter_params = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n",
    "        results.loc[i, :] = eval_lgb_results(iter_params, i)\n",
    "\n",
    "    # Sort by best score\n",
    "    results.sort_values(\"score\", ascending=False, inplace=True)\n",
    "    results.reset_index(inplace=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take full advantage of LightGBM's capabilities, we convert our training data into a LightGBM Dataset object. We\n",
    "then begin training a Gradient Boosting model on our data. The performance and behavior of Gradient\n",
    "Boosting models depends greatly on the value of certain model parameters, also known as hyperparameters. We could\n",
    "pick these values at random and hope for the best, but that would be an uncertain and unreliable process. Instead,\n",
    "we'll first define a range for many of those parameters, and then use a fifteen-fold Randomized Search to zero in on a\n",
    "strong set of candidates. Each iteration of the search will select a random from our defined ranges for each param,\n",
    "and then conduct k-fold cross-validation, keeping track of each iteration's score using the ROC-AUC metric.\n",
    "\n",
    "The randomized search should take ~12 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = lgb.Dataset(data=X_train, label=y_train)\n",
    "\n",
    "param_grid = {\n",
    "    'boosting_type': ['gbdt'],\n",
    "    'num_leaves': list(range(20, 150)),\n",
    "    'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base=10, num=1000)),\n",
    "    'subsample_for_bin': list(range(20000, 300000, 20000)),\n",
    "    'min_child_samples': list(range(20, 500, 5)),\n",
    "    'reg_alpha': list(np.linspace(0, 1)),\n",
    "    'reg_lambda': list(np.linspace(0, 1)),\n",
    "    'colsample_bytree': list(np.linspace(0.6, 1, 10)),\n",
    "    'subsample': list(np.linspace(0.5, 1, 100)),\n",
    "    'is_unbalance': [False],\n",
    "    \"first_metric_only\": [True]\n",
    "}\n",
    "\n",
    "random_results = light_random_search(param_grid, max_evals=15)\n",
    "print('The best validation score was {:.5f}'.format(random_results.loc[0, 'score']))\n",
    "print('\\nThe best hyperparameters were:')\n",
    "\n",
    "pprint(random_results.loc[0, 'params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After our fifteen iterations of Random Search, we train a new model with the best set of parameters identified, and test\n",
    "it against the data we held out earlier, which the model has not seen. We'll also take a look at some visualizations to get a better sense of what's happening here, specifically:\n",
    "\n",
    "- Which features are contributing most to the decisions the model is making\n",
    "- A Confusion Matrix breaking down the precision, recall, and f1 score of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best params from the random search\n",
    "rsearch_params = random_results.loc[0, \"params\"]\n",
    "\n",
    "# Create, train, and test model with the derived params\n",
    "model = lgb.LGBMClassifier(**rsearch_params, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "roc_auc = roc_auc_score(y_test, preds)\n",
    "pr_score = average_precision_score(y_test, preds)\n",
    "\n",
    "print(\"The best model from random search scores {:.5f} ROC-AUC on the test set and has an average precision of {:.5f}\"\n",
    "      .format(roc_auc, pr_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "importances = model.feature_importances_\n",
    "tuples = sorted(zip(column_labels, importances), key=lambda x: x[1])\n",
    "\n",
    "# Strip out features with zero importance\n",
    "tuples = [x for x in tuples if x[1] > 0]\n",
    "feature_names, values = zip(*tuples)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n",
    "ax1.set_title(\"Random Search Feature Importances\")\n",
    "ax1.barh(np.arange(len(values)), values, align=\"center\")\n",
    "ax1.set_yticks(np.arange(len(values)))\n",
    "ax1.set_yticklabels(feature_names)\n",
    "ax1.set_xlim(0, max(values) * 1.1)\n",
    "ax1.set_ylim(-1, len(values))\n",
    "ax1.set_xlabel(\"Feature Importance\")\n",
    "ax1.set_ylabel(\"Features\")\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "labels = ['Male', 'Female']\n",
    "sns.heatmap(cm, xticklabels=labels, yticklabels=labels, annot=True, fmt='d', cmap=\"Blues\", vmin=0.2)\n",
    "ax2.set_title(\"Confusion Matrix\")\n",
    "ax2.set_ylabel(\"Ground Truth\")\n",
    "ax2.set_xlabel(\"Predictions\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above visualizations, we can see that the use of punctuation played a very strong role in the model's decisions - the top three features in terms of importance are punctation-related!\n",
    "\n",
    "Digging a little deeper, we can leverage the SHaply Additive exPlanations (SHAP) library to get a sense of how the various features we've collected push the model to make a prediction of a user's gender. This library calculates Shaply values for all the features we've used, which captures the average marginal contribution of each feature to the model's predictions. To cut down on processing times, we'll just examine the predictions that the model was most confident of, i.e. above 98% confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "high_probability_preds - X_test[preds >= 0.98]\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    expected_value = explainer.expected_value\n",
    "    shap_values = explainer.shap_values(T)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a broader look at the impact of all the features in one. We can visualize this using a SHAP summary plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, features, feature_names=column_labels, class_names=[\"Male\", \"Female\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, we can see that several features, shown here at the bottom of the plot, had virtually no impact on the classification, and can likely be safely discarded in futrue iterations. Towards the top, we can get a sense for how a higher value for a given feature affected the model's choice.\n",
    "\n",
    "For example, let's look at the top line. The blue dots indicate a low value for the chosen feature, in this case num_adverbs. We can see there is a concentrated blob of blue on the left-hand side, indicating that the model believes lower numbers of adverbs is indicative of a male author (male was coded to 0 in our target data). Conversely, we can see that all the samples with higher numbers of adverbs are on the right-hand side, indicating a prediction of female. Interestingly, although the effect on the model's prediction is relatively weaker, we can see clear high/low divides on either side of the line for several other features, including positive sentiment, tweet length, and number of unique words used.\n",
    "\n",
    "Next, let's use our focus on the samples the model is most confident about and plot a SHAP decision plot, showing our model's typical prediction paths. In this way we can see what the model's various predictions have in common and what stands out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.decision_plot(expected_value, shap_values, T, feature_order=\"hclust\", link=\"logit\", feature_names=column_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirming the magnitude of the feature rankings above, we can see that the top few features exert almost all the predictive force in the model. In particular, starting at num_mentions, we can see the predictions diverge sharply. Given the clustering of predictions and the slope of the lines, it seems fair to say that num_hashtags and num_conjunctions are strongly correlated with a prediction of \"male\", while num_nouns gives the strongest push in the direction of a prediction of \"female\". Given these results, higher values of num_hashtags and num_conjunctions seem indicative of a male author, while higher values of num_nouns indicates a female author.\n",
    "\n",
    "To wrap up, as we can see in the readout of our predictions above, our model got a score of just under 0.80 ROC-AUC on the test set, while scoring over 0.80 in cross-validation. Not bad! In a perfect world with a much more powerful computer available, we could do a lot better by feeding the results of our random search into an exhaustive grid search. While I will not implement this here due to the computation and time cost, we can sketch out how it would work.\n",
    "\n",
    "Starting from the best hyperparameter values derived from random search, we would define new, focused ranges around\n",
    "each one, and feed that grid of params into an exhaustive search. In the cell below we define a helper method that\n",
    "would conduct the search for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def light_grid_search(param_grid):\n",
    "    results = pd.DataFrame(columns=[\"score\", \"params\", \"iteration\"])\n",
    "\n",
    "    # Get every possible combination of params from the grid\n",
    "    keys, grid_vals = zip(*param_grid.items())\n",
    "\n",
    "    # Iterate over every possible combination of hyperparameters\n",
    "    for i, v in enumerate(product(*grid_vals)):\n",
    "        iter_params = dict(zip(keys, v))\n",
    "        results.loc[i, :] = eval_lgb_results(iter_params, i)\n",
    "\n",
    "    # Sort by best score\n",
    "    results.sort_values(\"score\", ascending=False, inplace=True)\n",
    "    results.reset_index(inplace=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure the results of the grid search would be directly comparable to their randomized counterparts, we'd use the\n",
    "same process of k-fold cross-validation. After the search finishes running, we'd follow the same steps we did above -\n",
    "taking the best hyperparameters, using them to train a model on our data, and then seeing how the model did against\n",
    "our held-out test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
